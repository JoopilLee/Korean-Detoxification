import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'NanumGothic'
import warnings
warnings.filterwarnings('ignore')
import streamlit  as st
import shap
import scipy as sp

import torch
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# ============================================< Classification >============================================
class Classifier:
    def __init__(self,tokenzier,model,device):         
        # GPU ì„¤ì •
        self.device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')

        # í† í¬ë‚˜ì´ì§•
        self.tokenizer = tokenzier

        # ëª¨ë¸ ìƒì„±
        self.model = model

        #device ì§€ì •
        self.device = device
    
    # ëª¨ë¸ ì˜ˆì¸¡
    # 0: curse, 1: non_curse
    def sentence_predict(self,sent):
        # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
        self.model.eval()

        # ì…ë ¥ëœ ë¬¸ì¥ í† í¬ë‚˜ì´ì§•
        tokenized_sent = self.tokenizer(
            sent,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=128
        )
        
        # ëª¨ë¸ì´ ìœ„ì¹˜í•œ GPUë¡œ ì´ë™ 
        tokenized_sent.to(self.device)

        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(
                input_ids=tokenized_sent["input_ids"],
                attention_mask=tokenized_sent["attention_mask"],
                token_type_ids=tokenized_sent["token_type_ids"]
                )

        # ê²°ê³¼ return
        logits = outputs[0]
        logits = logits.detach().cpu()
        result = logits.argmax(-1)
        if result == 0:
            result = " >> ì•…ì„±ëŒ“ê¸€ ğŸ‘¿"
        elif result == 1:
            result = " >> ì •ìƒëŒ“ê¸€ ğŸ˜€"
        return result

# ============================================< XAI >============================================
# SHAP
# define a prediction function
class SHAP:
    def __init__(self,tokenizer,model,device) -> None:
          self.tokenizer = tokenizer
          self.model = model
          self.explainer = shap.Explainer(self.f,self.tokenizer)
          self.device = device
          with open('/home/jupyter/Korean_Text_Detoxification/data/stopwords.txt', 'r', encoding='utf-8') as file:
            # íŒŒì¼ ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ì–´ì™€ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            lines = file.readlines()
            self.stopwords = [line.strip() for line in lines]

    def f(self,x):
        tv = torch.tensor([self.tokenizer.encode(v, pad_to_max_length=True, max_length=128, truncation=True) for v in x],device=self.device)
        outputs = self.model(tv)[0].detach().cpu().numpy()
        scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
        val = sp.special.logit(scores[:,1]) # use one vs rest logit units
        return val
    # build an explainer using a token masker
    def get_shapevalue(self,sentence):
        return self.explainer([sentence])

    def sort_shape_value(self,sentence,shap_values):
        print('-'*100)
        # í† í°í™” ê²°ê³¼
        print('í† í°í™” ê²°ê³¼:', self.tokenizer.tokenize(sentence))
        # ë¶€ì •ì ì¸ ì˜í–¥ì„ í¬ê²Œ ë¯¸ì¹˜ëŠ” ìˆœì„œëŒ€ë¡œ í† í° ì •ë ¬
        print('-'*100)
        shap_values_list = list(zip(shap_values[0].values, shap_values[0].data))
        shap_values_list.sort(key=lambda x: x[0])
        for shap_value, feature in shap_values_list:
            if shap_value == 0.0:
                continue
            else:
                print(feature, shap_value)
        print('-'*100)
         
    def masking(self,sentence):
        shap_values = self.explainer([sentence])
        threshold = sum([i for i in shap_values[0].values if i < 0]) / len([i for i in shap_values[0].values if i < 0])
        # ë§ˆìŠ¤í‚¹
        mask_list = []
        shap_values_zip = zip(shap_values[0].values, shap_values[0].data)
        for shap_value, feature in shap_values_zip:
            if shap_value < threshold: # threshold
                if feature.strip() not in self.stopwords: # stopwords ì œê±°
                    if feature.strip().isalnum(): # íŠ¹ìˆ˜ë¬¸ì ì œê±°
                        mask_list.append(feature.strip())
        print('-'*100)
        print('masking list:', mask_list)
        print(f"ì›ë³¸ ë¬¸ì¥ : {sentence}")
        ori = sentence
        for mask in mask_list:
            sentence = sentence.replace(mask, "[mask]")

        return ori, sentence


# ============================================< Masking >============================================
